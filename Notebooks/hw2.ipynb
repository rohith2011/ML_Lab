{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name, email and UFID.\n",
    "Please do not modify instruction cells or any cells with automated tests (marked with `[ASSERTS]`). Note: you can add new cells if you need them, but answers must be in the cells with `YOUR CODE HERE` or \"YOUR ANSWER HERE\" comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d32c3222b2442d52b065e77cd55ac2f",
     "grade": false,
     "grade_id": "homework-preamble",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Homework 2: Regression and Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5658c9a4ed21f362746fdaeb7fb21351",
     "grade": false,
     "grade_id": "preamble-name",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Preamble: Write your Name, Email and UFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cac6c2934a26e1ec9725344dae3cb163",
     "grade": false,
     "grade_id": "name-email-ufid",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homework 2 -- name: Kondreddy Rohith Sai Reddy, email: r.kondreddy@ufl.edu, UFID: 65682267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NAME = 'Kondreddy Rohith Sai Reddy'\n",
    "EMAIL = 'r.kondreddy@ufl.edu'\n",
    "UFID = 65682267\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "print('Homework 2 -- name: {}, email: {}, UFID: {}\\n'.format(NAME, EMAIL, UFID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c41a13a5f6e2fa1b47621d28e926c2cf",
     "grade": true,
     "grade_id": "name-email-ufid-asserts",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check that your name, email, and UFID is filled in.\"\"\"\n",
    "assert NAME != '' and NAME != 'Your name here.' and len(NAME) > 3\n",
    "assert EMAIL != '' and EMAIL != 'Your email here.' and len(EMAIL) > 7\n",
    "assert type(UFID) == int and UFID != 12345678 and UFID >= 10000000 and UFID <= 99999999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b4f30c4bbc2f9927b1410eeb1f001b3",
     "grade": false,
     "grade_id": "preamble-academic-integrity",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Academic Integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99184dabc791053131230787b9f498b8",
     "grade": false,
     "grade_id": "preamble-academic-integrity-2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### <span style=\"color:red;\">This is an individual assignment. Academic integrity violations (i.e., cheating, plagiarism) will be reported to SCCR!</span><br/>\n",
    "#### The official CISE policy recommended for such offenses is a course grade of E. Additional sanctions may be imposed by SCCR such as marks on your permanent educational transcripts, dismissal or expulsion.\n",
    "#### Reminder of the Honor Pledge: On all work submitted for credit by Students at the University of Florida, the following pledge is either required or implied: *\"On my honor, I have neither given nor received unauthorized aid in doing this assignment.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d8d2452e1d9f6d547eae6447b7ca369",
     "grade": false,
     "grade_id": "cell-preamble-academic-integrity-3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Acknowledgement: Do you acknowledge and understand the academic integrity warning above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89bc9ed2e09cb9069b92dc24a3bc081a",
     "grade": false,
     "grade_id": "academic-integrity",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "academic_integrity_acknowledgement = True\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7d6eb103ab3a60e964c163468d9aa7a",
     "grade": true,
     "grade_id": "academic-integrity-assert",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check that you acknowledge the academic integrity warning, you understand it and have been reminded of the UF Honor Pledge.\"\"\"\n",
    "assert academic_integrity_acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f73a6c8f764fdac6667f0157a544866",
     "grade": false,
     "grade_id": "task1-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# [Task 1] (20 points) Loading and preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd89203347fe0a5164b6775ddd443acb",
     "grade": false,
     "grade_id": "task1-instructb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 1] We will use the Bike Sharing dataset (hourly). A version of this dataset is included in the homework handout archive.\n",
    "### This dataset contains features of users bike sharing/rental on an hourly basis.\n",
    "### The task is to predict how many users are sharing/renting a bike.\n",
    "### In this task you will load the data and preprocess it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40e0c6343e3d6ae8ef5568125e9de64f",
     "grade": false,
     "grade_id": "task1-instructc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### The following cell's code (import statements etc.) is provided for you and you should not need to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "119d3a62a34e7425f288493659994237",
     "grade": false,
     "grade_id": "task1-code",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "### Python version: 3.11.7 (v3.11.7:fa7a6f2303, Dec  4 2023, 15:22:56) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "### NumPy version: 1.26.3\n",
      "### Scikit-learn version: 1.3.2\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# Load packages we need\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# Let's check our software versions\n",
    "print('------------')\n",
    "print('### Python version: ' + __import__('sys').version)\n",
    "print('### NumPy version: ' + np.__version__)\n",
    "print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "print('------------')\n",
    "\n",
    "def var_exists(var_name):\n",
    "    return (var_name in globals() or var_name in locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5cc6d3b909b447a027649ca1d83883d9",
     "grade": false,
     "grade_id": "seed_instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### This is the seed we will use, do not change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d04202483cecc7aa53f8ad98656ef967",
     "grade": false,
     "grade_id": "setting_seed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "prop_vec = [16, 2, 2] # proportions for train - val - test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d746a78509c270bb1f51eff6a455a1f6",
     "grade": true,
     "grade_id": "seed_checking",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check seed. \"\"\"\n",
    "assert seed == 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b26690ee65de9a82553bc05a151c027e",
     "grade": false,
     "grade_id": "task1-instructd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Loading data (set the path correctly so it runs on your machine --- don't submit the data file with your notebook).\n",
    "#### Note: this dataset has missing values (artificially introduced), which you'll need to fill in before you can train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "424a20cc93e7cd774f4c8c7ca4e10b8e",
     "grade": false,
     "grade_id": "task1-loaddata",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Fill in the path to the directory where 'bikesharehour.csv.gz' is located.\n",
    "\"\"\"\n",
    "data_root = '/Users/rohithreddy/Desktop/Assaignments/MLE/hw2/data/' #put the path here\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f165779998390d2ddff4aea0e0ac8453",
     "grade": true,
     "grade_id": "task1-loaddata-test",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17379 entries, 0 to 17378\n",
      "Data columns (total 15 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   season      16320 non-null  float64\n",
      " 1   year        16231 non-null  float64\n",
      " 2   month       16304 non-null  float64\n",
      " 3   hour        16254 non-null  float64\n",
      " 4   holiday     16277 non-null  float64\n",
      " 5   weekday     16282 non-null  float64\n",
      " 6   workingday  16297 non-null  float64\n",
      " 7   weathersit  16324 non-null  float64\n",
      " 8   temp        16242 non-null  float64\n",
      " 9   atemp       16271 non-null  float64\n",
      " 10  hum         16252 non-null  float64\n",
      " 11  windspeed   16281 non-null  float64\n",
      " 12  registered  16244 non-null  float64\n",
      " 13  nsqrtc      16263 non-null  float64\n",
      " 14  count       17379 non-null  int64  \n",
      "dtypes: float64(14), int64(1)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_fp = os.path.join(data_root, 'bikesharehour.csv.gz')\n",
    "assert os.path.exists(dataset_fp), 'Dataset not found ({})!'.format(dataset_fp)\n",
    "df = pd.read_csv(dataset_fp, compression='gzip', header=0, na_values='?')\n",
    "\n",
    "# Check that we loaded the data as expected\n",
    "df_expected_shape = (17379, 15)\n",
    "assert df.shape == df_expected_shape, 'Unexpected shape of df!'\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2658b88a5895c78122a30edae6ae60e1",
     "grade": false,
     "grade_id": "task1a_instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 1a] (5 points) Show the first 8 rows of the dataframe df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c184f6d42bf2f1fd7c9d09dcb513c987",
     "grade": false,
     "grade_id": "task1a_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   season  year  month  hour  holiday  weekday  workingday  weathersit  temp  \\\n",
      "0     1.0   0.0    NaN   0.0      0.0      6.0         0.0         1.0   NaN   \n",
      "1     1.0   0.0    NaN   1.0      0.0      6.0         0.0         1.0   NaN   \n",
      "2     1.0   0.0    1.0   2.0      0.0      6.0         0.0         1.0   0.0   \n",
      "3     1.0   0.0    1.0   3.0      0.0      6.0         0.0         1.0   0.0   \n",
      "4     1.0   0.0    1.0   4.0      0.0      6.0         0.0         1.0   0.0   \n",
      "5     1.0   0.0    1.0   5.0      0.0      6.0         0.0         2.0   0.0   \n",
      "6     1.0   0.0    1.0   6.0      0.0      6.0         0.0         1.0   0.0   \n",
      "7     1.0   0.0    1.0   7.0      0.0      6.0         0.0         1.0   0.0   \n",
      "\n",
      "   atemp  hum  windspeed  registered  nsqrtc  count  \n",
      "0    0.0  0.0        0.0        13.0    -5.0     16  \n",
      "1    0.0  0.0        0.0        32.0    -8.0     40  \n",
      "2    0.0  0.0        0.0        27.0    -7.0     32  \n",
      "3    0.0  0.0        0.0        10.0    -5.0     13  \n",
      "4    0.0  0.0        0.0         1.0     0.0      1  \n",
      "5    0.0  0.0        1.0         1.0     0.0      1  \n",
      "6    0.0  0.0        0.0         0.0     4.0      2  \n",
      "7    NaN  0.0        0.0         2.0    -3.0      3  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"Put your code here.\n",
    "\"\"\"\n",
    "## what does the data look like?\n",
    "# YOUR CODE HERE\n",
    "# Show the first 8 rows of the dataframe df\n",
    "print(df.head(8))\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "207850a57807f6a443236f5a23ef531e",
     "grade": true,
     "grade_id": "task1-features",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: ['season', 'year', 'month', 'hour', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'registered', 'nsqrtc'] --- target: count\n"
     ]
    }
   ],
   "source": [
    "# grab all the data as a numpy matrix\n",
    "all_xy = df.to_numpy()\n",
    "\n",
    "col_names = [c for c in df.columns]\n",
    "features = col_names[:-1]\n",
    "target = col_names[-1]\n",
    "\n",
    "# what are the features and what is the target?\n",
    "print('features: {} --- target: {}'.format(features, target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c77734990f89e7420bea2fd71508fcc",
     "grade": false,
     "grade_id": "task1b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 1b] (5 points) Answer the following questions by setting variables (hardcoding or computing the answer) from 'all_xy'. (1) If we do supervise learning to predict 'count' based on our features is this classification or regression? (Set the corresponding variable to True.) (2) How many NaNs are there in columns 'holiday','temp', and 'count'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e10a22f5cd4a405b28c06b0d1bafc2a",
     "grade": false,
     "grade_id": "task1b-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1102\n",
      "1137\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Put your code here and set the variables appropriately.\n",
    "\"\"\"\n",
    "classification = False\n",
    "regression = False\n",
    "holiday_NaNs = -1\n",
    "temp_NaNs = -1\n",
    "count_NaNs = -1\n",
    "# YOUR CODE HERE\n",
    "\n",
    "classification = False\n",
    "regression = True\n",
    "\n",
    "holiday_NaNs = df['holiday'].isna().sum()\n",
    "temp_NaNs = df['temp'].isna().sum()\n",
    "count_NaNs = df['count'].isna().sum()\n",
    "\n",
    "print(holiday_NaNs)\n",
    "print(temp_NaNs)\n",
    "print(count_NaNs)\n",
    "\n",
    "# raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc7d99202916a313b25fcfe853eabb82",
     "grade": true,
     "grade_id": "task1b_tests",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 1b completed. \"\"\"\n",
    "\n",
    "assert var_exists('classification') and var_exists('regression') and classification != regression\n",
    "assert var_exists('holiday_NaNs') and holiday_NaNs >= 0\n",
    "assert var_exists('temp_NaNs') and temp_NaNs >= 0\n",
    "assert var_exists('count_NaNs') and count_NaNs >= 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4c3f1a8772ba51267b07033f0557d23",
     "grade": false,
     "grade_id": "task1c_instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 1c] (5 points) Let's impute the missing values! Use Scikit-learn's SimpleImputer to replace all NaNs in 'all_xy' with the *most frequent* value in each column. Use copy=True and store the results in 'all_xy_noNaNs' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ed4013643fc31a30fb6b4c395a42fa3",
     "grade": false,
     "grade_id": "task1c_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Put your code here (~2-3 lines).\n",
    "\"\"\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent', copy=True)\n",
    "all_xy_noNaNs = imputer.fit_transform(all_xy)\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4efd8da58007234bfbad9d825036ade0",
     "grade": true,
     "grade_id": "task1c_tests",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 1c completed. \"\"\"\n",
    "\n",
    "assert var_exists('all_xy_noNaNs') and all_xy_noNaNs.shape == df_expected_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1024eb6bf3e5eb85b038955feda0cbdc",
     "grade": false,
     "grade_id": "task1d-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 1d] (5 points) Min-max normalize the features (to [0,1] range) using sklearn's MinMaxScaler. Store the results into 'scaled_all_x'. Then split the data into train, val, test according to the proportion in prop_vec using sklearn's train_test split. Store the results into 'train_x', 'train_y', 'test_x', 'test_y', 'val_x', 'val_y'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33549a9e16d2ce1ac32610f23d0c8d16",
     "grade": false,
     "grade_id": "task1d_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Put your code here (~5-7 lines).\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "features = all_xy_noNaNs[:, :-1]\n",
    "target = all_xy_noNaNs[:, -1]\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_all_x = scaler.fit_transform(features)\n",
    "\n",
    "train_size = prop_vec[0] / sum(prop_vec)\n",
    "temp_test_size = 1 - train_size\n",
    "val_size = prop_vec[1] / (prop_vec[1] + prop_vec[2])\n",
    "train_x, temp_test_x, train_y, temp_test_y = train_test_split(scaled_all_x, target, train_size=train_size, random_state=seed)\n",
    "val_x, test_x, val_y, test_y = train_test_split(temp_test_x, temp_test_y, test_size=val_size, random_state=seed)\n",
    "\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f4391b903af2254d5c08592beb7f674",
     "grade": true,
     "grade_id": "task1d_tests",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check split for task 1d. \"\"\"\n",
    "assert var_exists('scaled_all_x')\n",
    "assert var_exists('train_x') and var_exists('train_y') and train_x.shape[0] == train_y.shape[0]\n",
    "assert var_exists('val_x') and var_exists('val_y') and val_x.shape[0] == val_y.shape[0]\n",
    "assert var_exists('test_x') and var_exists('test_y') and test_x.shape[0] == test_y.shape[0]\n",
    "assert train_x.shape == (13903, 14) and val_x.shape == (1738, 14) and test_x.shape == (1738, 14)\n",
    "assert np.amin(train_x) >= 0.0 and np.amax(train_x) <= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d244ec65844cdeb839f49b48a142b738",
     "grade": false,
     "grade_id": "task2_instructa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 2] (30 points) Evaluating and training linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "468d738a8668ff10566793c58904bae5",
     "grade": false,
     "grade_id": "task2a_instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 2a] (5 points) Fill in the code to calculate and return the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1d196c4244c95960b2734f67ab68df8",
     "grade": false,
     "grade_id": "task2a_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Fill in your code below (~5-7 lines).\n",
    "\"\"\"\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def r2_mse_mae_eval(model, tr_x, tr_y, v_x, v_y, pref='', verb=True):\n",
    "\n",
    "    \"\"\"Fill in your code below (~1-2 lines).\n",
    "    \"\"\"\n",
    "    # R^2 the coefficient of determination\n",
    "    # YOUR CODE HERE\n",
    "    train_r2 = r2_score(tr_y, model.predict(tr_x))\n",
    "    val_r2 = r2_score(v_y, model.predict(v_x))\n",
    "    \n",
    "    \n",
    "    if verb:\n",
    "        print('{}Train R^2: {:.3f}, Val  R^2: {:.3f}'.format(pref, train_r2, val_r2))\n",
    "\n",
    "    train_pred = model.predict(tr_x)\n",
    "    val_pred = model.predict(v_x)\n",
    "\n",
    "    \"\"\"Fill in your code below (~1-2 lines).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "        # Calculate Mean Squared Error (MSE)\n",
    "    train_mse = mean_squared_error(tr_y, train_pred)\n",
    "    val_mse = mean_squared_error(v_y, val_pred)\n",
    "    \n",
    "\n",
    "    if verb:\n",
    "        print('{}Train MSE: {:.3f}, Val MSE: {:.3f}'.format(pref, train_mse, val_mse))\n",
    "\n",
    "    \"\"\"Fill in your code below (~1-2 lines).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Calculate Mean Absolute Error (MAE)\n",
    "    train_mae = mean_absolute_error(tr_y, train_pred)\n",
    "    val_mae = mean_absolute_error(v_y, val_pred)\n",
    "    \n",
    "\n",
    "    if verb:\n",
    "        print('{}Train MAE: {:.3f}, Val MAE: {:.3f}'.format(pref, train_mae, val_mae))\n",
    "\n",
    "    return train_r2, val_r2, train_mse, val_mse, train_mae, val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18b3441ddafcf1fd86c8562815958ce4",
     "grade": true,
     "grade_id": "task2a_tests",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check split for task 2a. \"\"\"\n",
    "assert var_exists('r2_mse_mae_eval')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19220fc3ba3d815f8fbcb28390c0ccf7",
     "grade": false,
     "grade_id": "task2b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 2b] (5 points) Train a linear regression model using the default (hyper)parameters. Call the resulting trained model 'lrmodel'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dfe0fdc4f5aeae197e64d44281a5e16",
     "grade": false,
     "grade_id": "task2b_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LinearRegression] Train R^2: 0.841, Val  R^2: 0.845\n",
      "[LinearRegression] Train MSE: 5264.081, Val MSE: 5189.409\n",
      "[LinearRegression] Train MAE: 39.676, Val MAE: 38.642\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Put your code here (~1-2 lines).\n",
    "\"\"\"\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "lrmodel = LinearRegression()\n",
    "lrmodel.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "# raise NotImplementedError()\n",
    "\n",
    "_ = r2_mse_mae_eval(lrmodel, train_x, train_y, val_x, val_y, pref='[{}] '.format(lrmodel.__class__.__name__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4ff682ab212149d93df0732df296ce5",
     "grade": false,
     "grade_id": "task2b-instruct2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 2b] How good is that model? (A few sentences is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f28a10e885076e373cd3240aae6b68d",
     "grade": true,
     "grade_id": "cell-task2b_answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Remove the 'raise NotImplementedError' line. Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line. Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "## Answer: \n",
    "#\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "## Answer: \n",
    "\n",
    "# The linear regression model performs well on both the training and validation datasets with R^2 scores of 0.841 and 0.845 respectively. \n",
    "# These scores indicate that the model can explain a portion of the variation, in the target variable showing a fit to the data.\n",
    "\n",
    "# Additionally the MSE and MAE values support the models efficacy. The MSE values for the training set (5264.081) and validation set (5189.409) along with MAE values of 39.676 and 38.642 suggest that the model can predict values to the ones on average. \n",
    "# The consistency in performance metrics across both sets implies that the model generalizes well to data without overfitting.\n",
    "\n",
    "# In summary the linear regression model proves effective for this dataset by offering dependable forecasts, for bike rental numbers.\n",
    "\n",
    "\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c931713a37b5131c09838b45911da0e8",
     "grade": false,
     "grade_id": "task2c_instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 2c] (5 points) Fill in the code below to setup a grid search. Concatenate the train and val sets into 'search_x' and 'search_y'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6adb5fda3e1a8a6253453ee906e3add6",
     "grade": false,
     "grade_id": "task2c_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## some code to do a grid search and automatically train & evaluate the model with the best hyperparams.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def do_grid_search(model, param_grid, x, y):\n",
    "    gs = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error')\n",
    "    gs_res = gs.fit(x, y)\n",
    "    return  gs_res.best_params_\n",
    "\n",
    "\n",
    "def search_train_eval(model, param_grid, tr_x=train_x, tr_y=train_y, v_x=val_x, v_y=val_y):\n",
    "\n",
    "    \"\"\"Put your code here (~2-3 lines). Concatenate the train and val sets into 'search_x' and 'search_y'.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    search_x = np.concatenate((tr_x, v_x), axis=0)\n",
    "    search_y = np.concatenate((tr_y, v_y), axis=0)\n",
    "    \n",
    "    hyperparams = do_grid_search(model, param_grid, search_x, search_y)\n",
    "    \n",
    "    class_obj = type(model)\n",
    "    m = class_obj(**hyperparams).fit(tr_x, tr_y)\n",
    "    \n",
    "    cn = str(class_obj).split(\"'\")[1]\n",
    "    cn = cn.split('.')[-1]\n",
    "    print('{}({})'.format(cn, hyperparams))\n",
    "\n",
    "    r2_mse_mae_eval(m, tr_x, tr_y, v_x, v_y, pref='\\t')\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48519e6f82d7736ba46225ca36362d92",
     "grade": true,
     "grade_id": "task2c_tests",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check task 2c. \"\"\"\n",
    "assert var_exists('search_train_eval')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2dd2d328fca71fef762e80afcb4c533e",
     "grade": false,
     "grade_id": "task2d_instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 2d] (10 points) Take a look at the code of search_train_eval() and do_grid_search(). Answer the following questions: \n",
    "### 1. Can you do LAD regression with sklearn? Why or why not? (Justify your answer.) Hint: take a look at the sklearn documentation and the course slides.\n",
    "### 2. Why is the scoring function for the grid search 'neg_mean_squared_error' (as opposed to 'mean_squared_error')? \n",
    "### 3. Why is it okay to do the search over search_x and search_y which are the concatenation of the train and 'validation sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72e30d480e9d39db7c1b99aa78c2dbc9",
     "grade": true,
     "grade_id": "task2d_answer",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Remove the 'raise NotImplementedError' line. Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line. Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "### Hint: take a look at the documentation of scikit-learn and think.\n",
    "## Answer: \n",
    "#\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "## Answer:\n",
    "\n",
    "# 1. Yes you can apply LAD regression using sklearn. Not, with the LinearRegression class. Instead you can utilize the QuantileRegressor with the set at 0.5. \n",
    "# This method is similar to LAD regression as it focuses on minimizing the median of deviations between predicted and actual values, which's at the heart of LAD regression.\n",
    "\n",
    "# 2. The scoring function used in grid search is 'neg_mean_squared_error' because in Scikit learn higher scores are preferred for scoring functions. \n",
    "# Since MSE is a loss function where lower values indicate performance converting it to a value ('neg_mean_squared_error') aligns with this convention and allows grid search to optimize the score by minimizing MSE effectively.\n",
    "\n",
    "# 3. It's acceptable to conduct searches on 'search_x'. Search_y', which combine training and validation sets. This method enables a evaluation of model parameters across a larger dataset enhancing hyperparameter tuning through increased data utilization in grid search. \n",
    "# However maintaining a test set not involved in grid search or parameter tuning is crucial, for evaluating model performance and ensuring its ability to generalize well to data.\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64460c67c3a177ebf1ab32de263afbc6",
     "grade": false,
     "grade_id": "task2e_instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 2e] (5 points) Write the code below to do a grid search on a LassoLars model. Call the resulting model 'lassolars_model'. Be sure to tune the regularization constant and 'fit_intercept'. Make sure training converges and that you set the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af01b53b3bacacf8b6212b96621717c2",
     "grade": false,
     "grade_id": "task2e_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LassoLars({'alpha': 0.001, 'fit_intercept': True})\n",
      "\tTrain R^2: 0.841, Val  R^2: 0.845\n",
      "\tTrain MSE: 5264.095, Val MSE: 5189.456\n",
      "\tTrain MAE: 39.676, Val MAE: 38.642\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Put your code here (~2-3 lines).\n",
    "\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, LassoLars\n",
    "# YOUR CODE HERE\n",
    "\n",
    "param_grid_lassolars = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1,15, 30],\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "lassolars = LassoLars(random_state=seed)\n",
    "\n",
    "lassolars_model = search_train_eval(model=lassolars, param_grid=param_grid_lassolars)\n",
    "\n",
    "\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4e8cf326726c6095e9cfb210218ede0",
     "grade": true,
     "grade_id": "task2e_tests",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check task 2e. \"\"\"\n",
    "assert var_exists('lassolars_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad6f73eba9ec23c4ffb8c2f3c9b79b1a",
     "grade": false,
     "grade_id": "task3-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 3] (25 points) Let's train polynomial regression models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4fb5dfae82b37f31d9212710d252b6cb",
     "grade": false,
     "grade_id": "task3a-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 3a] (15 points) Use PolynomialFeatures to create a version of the data with all features of degree 3. Follow the provided instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3de785d8bb3ca7ddc296eb8b7968897",
     "grade": false,
     "grade_id": "task3a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge({'alpha': 0.1})\n",
      "\tTrain R^2: 0.842, Val  R^2: 0.845\n",
      "\tTrain MSE: 5204.200, Val MSE: 5184.758\n",
      "\tTrain MAE: 39.259, Val MAE: 38.452\n",
      "LinearRegression({'copy_X': True, 'fit_intercept': False, 'n_jobs': 1})\n",
      "\tTrain R^2: 0.949, Val  R^2: 0.948\n",
      "\tTrain MSE: 1686.084, Val MSE: 1736.016\n",
      "\tTrain MAE: 20.558, Val MAE: 21.580\n",
      "Ridge({'alpha': 1.0})\n",
      "\tTrain R^2: 0.934, Val  R^2: 0.933\n",
      "\tTrain MSE: 2176.000, Val MSE: 2258.630\n",
      "\tTrain MAE: 22.591, Val MAE: 23.686\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Put your code here (~10-15 lines).\n",
    "    1. Use PolynomialFeatures to create a version of the data with all features of degree 3. Make sure to allow interactions (interaction_only=False) and set include_bias=False.\n",
    "    Store the result in 'all_x_pf'. Ensure that you make a copy of the original data and you use the scaled features ('scaled_all_x')!\n",
    "    2. Split the data ('all_x_pf') into train-val-test using proportion from 'prop_vec' and save the result as 'train_{x,y}_pf', 'val_{x,y}_pf', and 'test_{x,y}_pf'.\n",
    "    3. Train three models for comparison. \n",
    "        (a) The first is a ridge regression model 'ridge' on the original scaled data (no polynomial features) where you tuned the hyperparameters.\n",
    "        (b) The second is a linear regression model 'lr_pf' on the polynomial features data.\n",
    "        (c) The third is a regularized \"linear\" model (ridge regression) on the polynomial features data where you tuned the hyperparameters (including regularization constant ensuring alpha >= 1.0).\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "all_x_pf = poly.fit_transform(scaled_all_x)\n",
    "\n",
    "\n",
    "param_grid = {'alpha': [0.1, 1.0, 10.0]}\n",
    "param_grid_lg = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'copy_X': [True, False],\n",
    "    'n_jobs': [1, -1]\n",
    "}\n",
    "param_grid_pf = {'alpha': [1.0, 10.0, 100.0]}\n",
    "\n",
    "\n",
    "train_prop = prop_vec[0] / sum(prop_vec)\n",
    "val_test_prop = prop_vec[1] / (prop_vec[1] + prop_vec[2])\n",
    "all_y=target\n",
    "\n",
    "train_x_pf, test_val_x_pf, train_y_pf, test_val_y_pf = train_test_split(all_x_pf, all_y, train_size=train_prop, random_state=seed)\n",
    "val_x_pf, test_x_pf, val_y_pf, test_y_pf = train_test_split(test_val_x_pf, test_val_y_pf, test_size=val_test_prop, random_state=seed)\n",
    "\n",
    "\n",
    "\n",
    "ridge_nopf=search_train_eval(Ridge(),param_grid,scaled_all_x, all_y)\n",
    "lr_pf=search_train_eval(LinearRegression(),param_grid_lg,train_x_pf, train_y_pf,val_x_pf,val_y_pf)\n",
    "ridge_pf=search_train_eval(Ridge(),param_grid_pf,train_x_pf, train_y_pf,val_x_pf,val_y_pf)\n",
    "\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a869fcd7d0c1fac1d6f305b5d52afaec",
     "grade": true,
     "grade_id": "task3a-tests",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Checks for task 3a. \"\"\"\n",
    "assert var_exists('all_x_pf') and all_x_pf.shape == (17379, 679)\n",
    "assert var_exists('train_x_pf') and var_exists('train_y_pf') and train_x_pf.shape[0] == train_y_pf.shape[0]\n",
    "assert var_exists('val_x_pf') and var_exists('val_y_pf') and val_x_pf.shape[0] == val_y_pf.shape[0]\n",
    "assert var_exists('test_x_pf') and var_exists('test_y_pf') and test_x_pf.shape[0] == test_y_pf.shape[0]\n",
    "assert train_x_pf.shape[0] == 13903 and val_x_pf.shape[0] == 1738 and test_x_pf.shape == val_x_pf.shape\n",
    "assert train_x_pf.shape[1] == val_x_pf.shape[1]\n",
    "assert np.amin(train_x_pf) >= 0.0 and np.amax(train_x_pf) <= 1.0\n",
    "\n",
    "assert var_exists('ridge_nopf') and var_exists('lr_pf') and var_exists('ridge_pf')\n",
    "assert ridge_nopf.coef_.shape == (14,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5417e88ed3d4f33f7626b96ece3e8ffe",
     "grade": false,
     "grade_id": "task3b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 3b] (5 points) For each of the three models, print the three most important features (coef values and name). You can use 'get_feature_names_out'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6425b4f6904d3ac77fc5e34757315941",
     "grade": false,
     "grade_id": "task3b_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge on original scaled data:\n",
      "Top 3 features:\n",
      "registered: 923.068313109986\n",
      "hour: 52.939201074128384\n",
      "workingday: -30.54761105202561\n",
      "\n",
      "LinearRegression on polynomial features data:\n",
      "Top 3 features:\n",
      "year workingday: -216110324760434.38\n",
      "year workingday^2: 108065030562325.53\n",
      "year^2 workingday: 108045294198085.11\n",
      "\n",
      "Ridge on polynomial features data:\n",
      "Top 3 features:\n",
      "registered^3: -659.9921427102967\n",
      "nsqrtc: -635.2285589709944\n",
      "registered: 534.1385102387685\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Put your code here (~5-7 lines).\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# def print_top_features(model, feature_names, n=3):\n",
    "#     # Get the coefficients from the model\n",
    "#     coefs = model.coef_\n",
    "#     # Get the top 'n' features with the highest absolute values of coefficients\n",
    "#     top_features_indices = np.argsort(np.abs(coefs))[-n:]\n",
    "#     # Print the feature names and their corresponding coefficients\n",
    "#     print(\"Top {} features:\".format(n))\n",
    "#     for i in top_features_indices:\n",
    "#         print(f\"{feature_names[i]}: {coefs[i]}\")\n",
    "\n",
    "# # For Ridge regression on the original scaled data\n",
    "# # Assuming you have a list/array of original feature names called 'original_feature_names'\n",
    "# print(\"Ridge regression on the original scaled data:\")\n",
    "# print_top_features(ridge_nopf, col_names[:-1])\n",
    "\n",
    "# # For Linear Regression on polynomial features data\n",
    "# print(\"\\nLinear Regression on polynomial features data:\")\n",
    "# poly_feature_names = poly.get_feature_names_out(input_features=col_names[:-1])  # Adjust 'original_feature_names' as necessary\n",
    "# print_top_features(lr_pf, poly_feature_names)\n",
    "\n",
    "# # For Ridge regression on polynomial features data\n",
    "# print(\"\\nRidge regression on polynomial features data:\")\n",
    "# print_top_features(ridge_pf, poly_feature_names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def print_top_features(model, feature_names, n=3):\n",
    "    coefs = model.coef_\n",
    "\n",
    "    if coefs.ndim > 1:\n",
    "        coefs = coefs.ravel()\n",
    "    top_features_indices = np.argsort(np.abs(coefs))[-n:][::-1]\n",
    "\n",
    "    print(\"Top {} features:\".format(n))\n",
    "    for idx in top_features_indices:\n",
    "        print(f\"{feature_names[idx]}: {coefs[idx]}\")\n",
    "\n",
    "def get_model_top_features(models_features, n=3):\n",
    "\n",
    "    for model, feature_names in models_features:\n",
    "        print(f\"\\n{type(model).__name__} on {'original scaled data' if 'Ridge' in type(model).__name__ and len(feature_names) <= len(col_names) else 'polynomial features data'}:\")\n",
    "        print_top_features(model, feature_names, n)\n",
    "\n",
    "original_feature_names = col_names[:-1]\n",
    "\n",
    "poly_feature_names = poly.get_feature_names_out(input_features=original_feature_names)\n",
    "\n",
    "models_features = [\n",
    "    (ridge_nopf, original_feature_names),\n",
    "    (lr_pf, poly_feature_names),\n",
    "    (ridge_pf, poly_feature_names)\n",
    "]\n",
    "\n",
    "get_model_top_features(models_features)\n",
    "\n",
    "\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bea6b39d98f12d1f62f27148d85ec62",
     "grade": true,
     "grade_id": "task3b_manual_answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" THIS CODE CELL IS INTENTIONALLY LEFT EMPTY.  Remove the 'raise NotImplementedError' line. You should leave this cell empty (it is used for autograding). (Do not change the cell type from code to markdown.)\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" THIS CODE CELL IS INTENTIONALLY LEFT EMPTY.  Remove the 'raise NotImplementedError' line. You should leave this cell empty (it is used for autograding). (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# THIS CODE CELL IS INTENTIONALLY LEFT EMPTY --- DO NOT MODIFY THIS CELL\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c62c7836558e3b4ee481de42b4f56d24",
     "grade": false,
     "grade_id": "task3c_instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 3c] (5 points) Out of the three models, which would you use and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b344b53283df484008b5ae14a68e8ae4",
     "grade": true,
     "grade_id": "task3c_manual_answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Remove the 'raise NotImplementedError' line. Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line. Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "## Answer: \n",
    "#\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# I prefer the Linear Regression model, with Polynomial Features because it has shown performance based on the following factors;\n",
    "#  It has the R^2 values (Train; 0.949 Val; 0.948) indicating a strong fit compared to other models.\n",
    "#  It also demonstrates the MSE and MAE suggesting predictions.\n",
    "#  The model performs consistently well on both training and validation datasets indicating generalization without overfitting.\n",
    "# Despite complexities its reliable predictive capabilities and ability to generalize effectively make it my top choice.\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "449dab32812541b0bb11bb3e3a649358",
     "grade": false,
     "grade_id": "task4-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 4] (25 points) Trees and Bagging Ensembles!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c85c9b2327b4e739bb17ea6c2145c1d",
     "grade": false,
     "grade_id": "task4-instruct2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's do some cleanup and sanity checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a12e338911260c668247d0041d1c859",
     "grade": false,
     "grade_id": "task4-instruct-code",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# let's do some cleanup and discard all the polynomial features stuff.\n",
    "if var_exists('all_x_pf'):\n",
    "    del all_x_pf, train_x_pf, train_y_pf, test_x_pf, test_y_pf, val_x_pf, val_y_pf\n",
    "\n",
    "# sanity check shapes\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape, val_x.shape, val_y.shape\n",
    "assert train_x.shape == (13903, 14) and val_x.shape == (1738, 14) and test_x.shape == (1738, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e0011ce50befe631de399acb915a494",
     "grade": false,
     "grade_id": "task4a-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 4a] (5 points) Suppose a sklearn decision tree has n nodes. How many total splits does it contain? (Hint: think of it as a CS question not an ML question. Also think about edge cases (e.g., n=0).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "695bdedb230ff2cea44d95bf364ab6a2",
     "grade": false,
     "grade_id": "task4a_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Put your code here to answer by implementing the function.\n",
    "\"\"\"\n",
    "def num_dt_splits(nodes): # should return the total number of splits in a decision tree. Must return an integer value.\n",
    "    assert nodes >= 0\n",
    "    # YOUR CODE HERE\n",
    "    if((nodes-1)<=0):\n",
    "        ret=True\n",
    "    return max(0,nodes-1)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da184360d058a9471fe3344d3dc64b19",
     "grade": true,
     "grade_id": "task4a-tests",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check task 4a. \"\"\"\n",
    "assert var_exists('num_dt_splits')\n",
    "assert num_dt_splits(0) == 0 and num_dt_splits(1) == 0\n",
    "for i in range(0, 100):\n",
    "    nodes = np.random.randint(0, 1e8)\n",
    "    splits = num_dt_splits(nodes)\n",
    "    assert splits>=0 and splits == int(splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae7566934d1bdc293f09c8fb02e3e788",
     "grade": false,
     "grade_id": "cell-282724082b83b395",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's train a decision tree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d562ad349530d56c05fe1c5974736c2",
     "grade": false,
     "grade_id": "task4b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 4b] (5 points) Train and evaluate two (regression) decision trees with sklearn's DecisionTreeRegressor. The first 'dtmodel' should be trained with default parameters (set the seed). The second 'dtregmodel' should be trained with the default parameters (set the seed) but a max depth of 8. Use r2_mse_mae_eval to print performance of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef9d31fa1e0b4b862b76d2ce94c3668f",
     "grade": false,
     "grade_id": "task4b-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of dtmodel (Default Parameters):\n",
      "[dtmodel] Train R^2: 1.000, Val  R^2: 0.978\n",
      "[dtmodel] Train MSE: 0.000, Val MSE: 741.429\n",
      "[dtmodel] Train MAE: 0.000, Val MAE: 9.714\n",
      "\n",
      "Performance of dtregmodel (Max Depth 8):\n",
      "[dtregmodel] Train R^2: 0.969, Val  R^2: 0.968\n",
      "[dtregmodel] Train MSE: 1042.599, Val MSE: 1067.890\n",
      "[dtregmodel] Train MAE: 15.293, Val MAE: 16.496\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Put your code here (~5 lines)\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "dtmodel = DecisionTreeRegressor(random_state=seed)\n",
    "dtmodel.fit(train_x, train_y)\n",
    "\n",
    "dtregmodel = DecisionTreeRegressor(max_depth=8, random_state=seed)\n",
    "dtregmodel.fit(train_x, train_y)\n",
    "\n",
    "print(\"Performance of dtmodel (Default Parameters):\")\n",
    "_ = r2_mse_mae_eval(dtmodel, train_x, train_y, val_x, val_y, pref='[dtmodel] ')\n",
    "\n",
    "print(\"\\nPerformance of dtregmodel (Max Depth 8):\")\n",
    "_ = r2_mse_mae_eval(dtregmodel, train_x, train_y, val_x, val_y, pref='[dtregmodel] ')\n",
    "\n",
    "\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef804154acc025a0f2dd335fd35fc345",
     "grade": true,
     "grade_id": "task4b-checks",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check task 4b. \"\"\"\n",
    "assert var_exists('dtmodel') and var_exists('dtregmodel') \n",
    "assert dtmodel.tree_.max_depth >= 8 and dtregmodel.tree_.max_depth == 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9de52d5ce39f0ee93b9ea9c7ac11c9f",
     "grade": false,
     "grade_id": "task4c-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 4c] (5 points) Is the regularized model overfitted? Is it better than the models trained in Tasks 2 and 3? (A few sentences suffice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff373393b63e44dd706dd02be10f635c",
     "grade": true,
     "grade_id": "task4c_manual_answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Remove the 'raise NotImplementedError' line. Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line. Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "## Answer: \n",
    "#\n",
    "# YOUR CODE HERE\n",
    "\n",
    "## Answer: \n",
    "\n",
    "# The decision tree model, with regularized parameters (Maximum Depth set to 8) shows performance without signs of overfitting maintaining R^2 values and error metrics across both training and validation datasets. \n",
    "# However the Linear Regression model incorporating Polynomial Features from Task 3 outperforms it by achieving Mean Squared Error (MSE) and Mean Absolute Error (MAE) indicating accuracy and generalization. \n",
    "# Therefore opting for the model from Task 3 which strikes a balance, between high R^2 values and low error metrics is the preferred choice.\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60d37ad914f87535f1c9bb16bb9526f7",
     "grade": false,
     "grade_id": "cell-fb9d19cf2516c675",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's explore bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e8dd970d6ffe798592d102494514539",
     "grade": false,
     "grade_id": "task4d_instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 4d] (5 points) Train two ensemble models using sklearn's BaggingRegressor. Use default parameters but be sure to set the seed and set max_samples=0.5. The first ensemble 'lr_bagging' must use 'LinearRegression' as weak learners, whereas the second 'dtr_bagging' must use 'DecisionTreeRegressor' as weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16e5d3a3b91e41285e5e9f093eb1f03c",
     "grade": false,
     "grade_id": "task4d_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Linear Regression Bagging] Train R^2: 0.841, Val  R^2: 0.845\n",
      "[Linear Regression Bagging] Train MSE: 5264.740, Val MSE: 5188.391\n",
      "[Linear Regression Bagging] Train MAE: 39.621, Val MAE: 38.568\n",
      "\n",
      "[DT Regression Bagging aka Random Forest] Train R^2: 0.990, Val  R^2: 0.985\n",
      "[DT Regression Bagging aka Random Forest] Train MSE: 335.826, Val MSE: 487.294\n",
      "[DT Regression Bagging aka Random Forest] Train MAE: 5.693, Val MAE: 8.144\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Put your code here (~3 lines)\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "lr_bagging = BaggingRegressor(estimator=LinearRegression(), n_estimators=10, max_samples=0.5, random_state=42)\n",
    "lr_bagging.fit(train_x, train_y)\n",
    "dtr_bagging = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, max_samples=0.5, random_state=42)\n",
    "dtr_bagging.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "# raise NotImplementedError()\n",
    "\n",
    "_ = r2_mse_mae_eval(lr_bagging, train_x, train_y, val_x, val_y, pref='[Linear Regression Bagging] ')\n",
    "print()\n",
    "_ = r2_mse_mae_eval(dtr_bagging, train_x, train_y, val_x, val_y, pref='[DT Regression Bagging aka Random Forest] ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "527ec37234f653709a3f8c57a3febf74",
     "grade": true,
     "grade_id": "task4d_tests",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check task 4d. \"\"\"\n",
    "assert var_exists('lr_bagging') and var_exists('dtr_bagging') \n",
    "assert lr_bagging.random_state == dtr_bagging.random_state and dtr_bagging.random_state == seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1380de7303df21fa79e0942ecf785d86",
     "grade": false,
     "grade_id": "task4e-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 4e] (5 points) Answer the following questions: \n",
    "### 1. Is the linear regression bagging ensemble a better model than linear regression (Task 2)? Is this expected? (We are looking for an explanation of why this approach performs the way it does.)\n",
    "### 2. How does the random forest compare to all other models in this and previous task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34e515e74c59752f118ce4e0a5c383c9",
     "grade": true,
     "grade_id": "task4e_manual_answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Remove the 'raise NotImplementedError' line. Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line. Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "## Answer: \n",
    "#\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# 1. The combination of Linear Regression, with Bagging demonstrates results to using Linear Regression on its own in Task 2 as expected. \n",
    "# Bagging is known for decreasing variance, which's more advantageous for models with variance. \n",
    "# Since Linear Regression typically has variance the benefits from bagging are limited as it mainly targets reducing variance than bias.\n",
    "\n",
    "# 2. Among all models assessed the Random Forest model stands out by surpassing both individual and combined Linear Regression models and decision tree models. \n",
    "# Its exceptional performance is credited to its ability to effectively minimize both bias and variance through learning making it the precise and broadly applicable model, among those examined.\n",
    "\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3499cc9afa0dbb2dd6b9244b3eaba066",
     "grade": false,
     "grade_id": "task5_instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# [Task 5] \\<*For CAI6108MLE Only*\\> (25 points) Stacking & MoE. For this task you will implement a (kind of) Stacking/MoE approach. \n",
    "# Roughly speaking the approach will be to randomly choose weak learners out of a pool of candidates (determined by the *last* digit of your UFID) and train each one of a randomly selected subset of features. With the weak learners trained, we will train a gating model to learn weights to combine the weak learners' predictions into a single final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "293408be685634ca1bdd7aa55dc4c9dd",
     "grade": false,
     "grade_id": "task5_instruct2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 5] The following code is provided, you should not modify it. But you should *read* it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c9d9d08db7f4bf716eac83bae4a0f0b",
     "grade": false,
     "grade_id": "task5_provided_code",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, LassoLars\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "\"\"\" Returns candidate weak learners based on UFID digit.\n",
    "\"\"\"\n",
    "def weak_learner_candidates(ufid_digit):\n",
    "    if ufid_digit in [1, 3, 6]:\n",
    "        return [SVR(kernel='poly'), Ridge(), DecisionTreeRegressor(max_depth=10)]\n",
    "    elif ufid_digit in [2, 4, 7]:\n",
    "        return [SVR(kernel='poly'), LassoLars(), KNeighborsRegressor(n_neighbors=3)]\n",
    "    else:\n",
    "        assert ufid_digit in [0, 5, 8, 9] \n",
    "        return [DecisionTreeRegressor(max_depth=8), Ridge(), KNeighborsRegressor(n_neighbors=1)]\n",
    "\n",
    "\"\"\" Randomly sample and return a weak learner out of the candidate pool.\n",
    "\"\"\"\n",
    "def random_weak_learner(candidates_fn):\n",
    "    assert candidates_fn is not None\n",
    "\n",
    "    candidates = candidates_fn()\n",
    "    assert len(candidates) == 3\n",
    "    ridx = np.random.randint(0, len(candidates))\n",
    "\n",
    "    return candidates[ridx]\n",
    "\n",
    "\"\"\" Predicts the value(s) on 'x' given the weak_learners and gating function. (Can you figure out how it works??)\n",
    "\"\"\"\n",
    "def predict(weak_learners, gating, x, epsilon=1e-4):\n",
    "    k = len(weak_learners)\n",
    "    \n",
    "    gating_weights = np.ones((x.shape[0], k)) * epsilon\n",
    "    gw = gating.predict_proba(x)\n",
    "    for i, cidx in enumerate(gating.classes_):\n",
    "        gating_weights[:,cidx] = gw[:,i]\n",
    "    gating_weights = gating_weights / np.sum(gating_weights, axis=-1).reshape((-1,1))\n",
    "    \n",
    "    pred_y = np.zeros((x.shape[0],))\n",
    "    for i, tup in enumerate(weak_learners):\n",
    "        weak_learner, sel_features = tup\n",
    "        predi = weak_learner.predict(x[:,sel_features])\n",
    "        pred_y += gating_weights[:,i] * predi\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8f0057245f1024a6d53c389b3486321",
     "grade": false,
     "grade_id": "task5a-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 5a] (10 points) Fill in the code of 'train_ensemble' and 'train_gating' following the provided instructions. Note: instructions do not spell out what every line of code you write should do --- that is for you to figure out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bf9901cd611d84172e788d165f9eaba",
     "grade": false,
     "grade_id": "task5a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Trains the ensemble of 'num_weak_learners' of randomly chosen weak learners, each selecting a random subset of 'num_sel_features' features.\n",
    "    Returns a list of tuples composed of the trained weak learner ('weak_learner') and the chosen features ('fidx).\n",
    "\"\"\"\n",
    "def train_ensemble(tr_x, tr_y, candidates_fn, num_weak_learners=16, num_sel_features=7):\n",
    "    ret = []\n",
    "    num_features = train_x.shape[1]\n",
    "    for i in range(0, num_weak_learners):\n",
    "\n",
    "        \"\"\"Put your code here (~3-4 lines)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        weak_learner = random_weak_learner(candidates_fn)\n",
    "        fidx = np.random.choice(num_features, num_sel_features, replace=False)\n",
    "        weak_learner.fit(tr_x[:, fidx], tr_y)\n",
    "\n",
    "\n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        weak_learner_tup = (weak_learner, fidx)\n",
    "        ret.append(weak_learner_tup)\n",
    "    return ret\n",
    "\n",
    "\n",
    "\"\"\" Trains the gating function (logistic regression) using the provided list of weak learners tuples.\n",
    "    Returns the gating model, errors matrix and best weak learner idx.\n",
    "\"\"\"\n",
    "def train_gating(weak_learners, tr_x, tr_y):\n",
    "    k = len(weak_learners)\n",
    "    errors = np.ones((tr_x.shape[0], k)) * np.inf\n",
    "    \"\"\"Put your code here (~3-4 lines). For each weak learner measure its *absolute errors* on tr_{x,y} (put that in 'errors').\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    for i, (weak_learner, fidx) in enumerate(weak_learners):\n",
    "        predictions = weak_learner.predict(tr_x[:, fidx])\n",
    "        abs_errors = np.abs(predictions - tr_y)\n",
    "        errors[:, i] = abs_errors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    bestidx = np.argmin(errors, axis=-1) # compute the weak learning idx with the lowest error\n",
    "    \n",
    "    # train a logistic regression model for the gating\n",
    "    class_weight = {i:1.0 for i in range(0, k)}\n",
    "    gating = LogisticRegression(penalty='l2', C=1.0, random_state=seed, max_iter=10000, class_weight=class_weight, multi_class='multinomial')\n",
    "    gating.fit(tr_x, bestidx)\n",
    "\n",
    "    return gating, errors, bestidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51c93ac8ed11f6fa777471dc8dabdfcd",
     "grade": true,
     "grade_id": "task5a-checks",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check task 5a. \"\"\"\n",
    "assert var_exists('weak_learner_candidates') and var_exists('random_weak_learner') \n",
    "assert var_exists('train_gating') and var_exists('train_ensemble') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfce67c6e346df2c380433e8ff8b63eb",
     "grade": false,
     "grade_id": "task5b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 5b] (10 points) Fill in the provided code. Use the functions you previously implemented to train the ensemble and the gating models and make predictions on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a92c9bec3d92d5b7d18a3bc037d9534",
     "grade": false,
     "grade_id": "task5b-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] R^2: 0.890, MSE: 3681.4, MAE: 39.9 [Elapsed: 11.1 seconds]\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "\"\"\"Put your code here (~3-4 lines). \n",
    "    1. Split the training data into s1_{x,y} and s2_{x,y} such that 70% ends up in s1 (and 30% in s2).\n",
    "    2. train the ensemble on s1_{x,y} [do not forget to use the *last* digit of your UFID].\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "s1_x, s2_x, s1_y, s2_y = train_test_split(train_x, train_y, test_size=0.3, random_state=42)\n",
    "\n",
    "ufid_last_digit = UFID%10\n",
    "# print(ufid_last_digit)\n",
    "wls = train_ensemble(s1_x, s1_y, lambda: weak_learner_candidates(ufid_last_digit))\n",
    "# wls = train_ensemble(s1_x, s1_y, lambda: weak_learner_candidates(ufid_last_digit), num_weak_learners=16, num_sel_features=7)\n",
    "\n",
    "\n",
    "\n",
    "# raise NotImplementedError()\n",
    "\n",
    "# train the gating on s2_{x,y}\n",
    "gating, errors, bestidx = train_gating(wls, s2_x, s2_y)\n",
    "# use the predict function to make predictions on the validation data\n",
    "pred_y = predict(wls, gating, val_x)\n",
    "\n",
    "\"\"\"Put your code here (~3-4 lines). \n",
    "    compute the R^2, MSE, MAE on the validation data (using 'pred_y') and store the result as 'val_r2', 'val_mse', 'val_mae'\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "val_r2 = r2_score(val_y, pred_y)\n",
    "val_mse = mean_squared_error(val_y, pred_y)\n",
    "val_mae = mean_absolute_error(val_y, pred_y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# raise NotImplementedError()\n",
    "\n",
    "et = time.time()\n",
    "elapsed = et - st\n",
    "print('[Val] R^2: {:.3f}, MSE: {:.1f}, MAE: {:.1f} [Elapsed: {:.1f} seconds]'.format(val_r2, val_mse, val_mae, elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "797a5b0af1bcbb19f897190a86dc79b9",
     "grade": true,
     "grade_id": "task5b_tests",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check task 5b. \"\"\"\n",
    "\n",
    "assert var_exists('gating')\n",
    "assert elapsed < 300.0\n",
    "assert np.abs(val_r2 - 0.88) <= 0.12 and np.abs(val_mae - 30) <= 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18b847192d58dc3d9c4c4f1e993e0407",
     "grade": false,
     "grade_id": "task5c-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 5c] (5 points) Now observe that 'num_weak_learners' and 'num_sel_features' in train_ensemble can be considered hyperparameters. Write some code to tune them through a simple grid search and then print the best value for them and the performance of the resulting ensemble model. (For this question best means the one with lowest val R^2.) Is it better than previous models in this homework?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0338b34208be0df1b92b0ae90fbae45a",
     "grade": false,
     "grade_id": "task5c-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] R^2: 0.970, MSE: 1013.1, MAE: 18.2 [Best hyperparams: 16, 5]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Put your code here. Call the resulting best hyperparameters 'best_num_weak_learners' and 'best_num_sel_features'\n",
    "    Store the performance R^2, MSE, MAE as 'val_r2', 'val_mse', 'val_mae'\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# num_weak_learners_options = [10, 16, 20]\n",
    "# num_sel_features_options = [5, 7, 10]\n",
    "# best_val_r2 = -np.inf\n",
    "# best_val_mse = np.inf\n",
    "# best_val_mae = np.inf\n",
    "# best_num_weak_learners = None\n",
    "# best_num_sel_features = None\n",
    "\n",
    "# for num_weak_learners in num_weak_learners_options:\n",
    "#     for num_sel_features in num_sel_features_options:\n",
    "#         ensemble = train_ensemble(s1_x, s1_y, lambda: weak_learner_candidates(ufid_last_digit), \n",
    "#                                   num_weak_learners=num_weak_learners, num_sel_features=num_sel_features)\n",
    "\n",
    "#         gating, _, _ = train_gating(ensemble, s2_x, s2_y)\n",
    "#         pred_y = predict(ensemble, gating, val_x)\n",
    "#         current_val_r2 = r2_score(val_y, pred_y)\n",
    "#         current_val_mse = mean_squared_error(val_y, pred_y)\n",
    "#         current_val_mae = mean_absolute_error(val_y, pred_y)\n",
    "#         if current_val_r2 > best_val_r2:\n",
    "#             best_val_r2 = current_val_r2\n",
    "#             best_val_mse = current_val_mse\n",
    "#             best_val_mae = current_val_mae\n",
    "#             best_num_weak_learners = num_weak_learners\n",
    "#             best_num_sel_features = num_sel_features\n",
    "# val_r2 = best_val_r2\n",
    "# val_mse = best_val_mse\n",
    "# val_mae = best_val_mae\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def evaluate_model_configuration(num_weak_learners, num_sel_features, s1_x, s1_y, s2_x, s2_y, val_x, val_y):\n",
    "    ensemble = train_ensemble(s1_x, s1_y, lambda: weak_learner_candidates(ufid_last_digit), \n",
    "                              num_weak_learners=num_weak_learners, num_sel_features=num_sel_features)\n",
    "\n",
    "    gating, _, _ = train_gating(ensemble, s2_x, s2_y)\n",
    "    pred_y = predict(ensemble, gating, val_x)\n",
    "    current_val_r2 = r2_score(val_y, pred_y)\n",
    "    current_val_mse = mean_squared_error(val_y, pred_y)\n",
    "    current_val_mae = mean_absolute_error(val_y, pred_y)\n",
    "    \n",
    "    return current_val_r2, current_val_mse, current_val_mae\n",
    "\n",
    "num_weak_learners_options = [10, 16, 20]\n",
    "num_sel_features_options = [5, 7, 10]\n",
    "best_val_r2 = -np.inf\n",
    "best_val_mse = np.inf\n",
    "best_val_mae = np.inf\n",
    "best_num_weak_learners = None\n",
    "best_num_sel_features = None\n",
    "\n",
    "for num_weak_learners in num_weak_learners_options:\n",
    "    for num_sel_features in num_sel_features_options:\n",
    "        current_val_r2, current_val_mse, current_val_mae = evaluate_model_configuration(\n",
    "            num_weak_learners, num_sel_features, s1_x, s1_y, s2_x, s2_y, val_x, val_y\n",
    "        )\n",
    "        \n",
    "        if current_val_r2 > best_val_r2:\n",
    "            best_val_r2 = current_val_r2\n",
    "            best_val_mse = current_val_mse\n",
    "            best_val_mae = current_val_mae\n",
    "            best_num_weak_learners = num_weak_learners\n",
    "            best_num_sel_features = num_sel_features\n",
    "\n",
    "val_r2 = best_val_r2\n",
    "val_mse = best_val_mse\n",
    "val_mae = best_val_mae\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# raise NotImplementedError()\n",
    "print('[Val] R^2: {:.3f}, MSE: {:.1f}, MAE: {:.1f} [Best hyperparams: {}, {}]'.format(val_r2, val_mse, val_mae, best_num_weak_learners, best_num_sel_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2ee25aea7b3cdffb0705fd51f663f74",
     "grade": true,
     "grade_id": "task5c-checks",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check task 5c. \"\"\"\n",
    "\n",
    "assert var_exists('best_num_weak_learners') and var_exists('best_num_sel_features')\n",
    "assert np.abs(val_r2 - 0.92) <= 0.08 and np.abs(val_mae - 24) <= 24\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
